"""
This script runs batch image enhancement workflows using Temporal.

It parses image configuration, connects to the Temporal server, and
manages the execution of multiple image enhancement workflows in parallel or sequentially.
"""
import asyncio
import logging
from temporalio.client import Client
from workflows import ImageEnhancementWorkflow
from activities import ImageProcessingConfig, S3Location
import os
from dotenv import load_dotenv
import uuid
import json
from typing import List, Dict, Any
from concurrent.futures import as_completed
import time

# Load environment variables
load_dotenv()

# Configure logging
log_level = getattr(logging, os.getenv('LOG_LEVEL', 'INFO').upper(), logging.INFO)
logging.basicConfig(
    level=log_level,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def parse_image_list(images_config: str) -> List[Dict[str, str]]:
    """
    Parse image configuration from environment variable.
    Supports both JSON format and simple comma-separated format.
    
    JSON format:
    [{"source_bucket": "bucket1", "source_key": "image1.jpg", "dest_bucket": "bucket2", "dest_key": "enhanced_image1.jpg"}, ...]
    
    Simple format (uses default buckets):
    image1.jpg,image2.png,folder/image3.jpeg
    """
    if not images_config:
        return []
    
    # Try to parse as JSON first
    try:
        images = json.loads(images_config)
        if isinstance(images, list):
            return images
    except json.JSONDecodeError:
        pass
    
    # Parse as comma-separated list
    source_bucket = os.getenv('SOURCE_BUCKET', 'source-bucket')
    dest_bucket = os.getenv('DEST_BUCKET', 'dest-bucket')
    
    image_keys = [key.strip() for key in images_config.split(',') if key.strip()]
    images = []
    
    for key in image_keys:
        # Generate destination key by adding 'enhanced_' prefix
        dest_key = f"enhanced_{os.path.basename(key)}"
        if '/' in key:
            # Preserve folder structure
            folder = os.path.dirname(key)
            dest_key = f"{folder}/enhanced_{os.path.basename(key)}"
        
        images.append({
            "source_bucket": source_bucket,
            "source_key": key,
            "dest_bucket": dest_bucket,
            "dest_key": dest_key
        })
    
    return images

async def run_single_image_workflow(client: Client, config: ImageProcessingConfig, 
                                  image_config: Dict[str, str], task_queue: str, 
                                  enhancement_prompt: str) -> Dict[str, Any]:
    """
    Run workflow for a single image.
    """
    source_location = S3Location(
        bucket=image_config['source_bucket'], 
        key=image_config['source_key']
    )
    dest_location = S3Location(
        bucket=image_config['dest_bucket'], 
        key=image_config['dest_key']
    )
    
    # Generate a unique workflow ID
    workflow_id = f"image-enhancement-{uuid.uuid4()}"
    
    logger.info(f"Starting workflow for: s3://{source_location.bucket}/{source_location.key}")
    
    try:
        # Start the workflow
        handle = await client.start_workflow(
            ImageEnhancementWorkflow.run,
            args=[config, source_location, dest_location, enhancement_prompt],
            id=workflow_id,
            task_queue=task_queue,
        )
        
        # Wait for workflow to complete and get result
        start_time = time.time()
        result = await handle.result()
        end_time = time.time()
        
        return {
            "workflow_id": workflow_id,
            "source": f"s3://{source_location.bucket}/{source_location.key}",
            "destination": f"s3://{dest_location.bucket}/{dest_location.key}",
            "status": "success",
            "result": result,
            "duration_seconds": round(end_time - start_time, 2)
        }
        
    except Exception as e:
        logger.error(f"Error processing {source_location.bucket}/{source_location.key}: {e}")
        return {
            "workflow_id": workflow_id,
            "source": f"s3://{source_location.bucket}/{source_location.key}",
            "destination": f"s3://{dest_location.bucket}/{dest_location.key}",
            "status": "failed",
            "error": str(e),
            "duration_seconds": 0
        }

async def run_batch_image_workflows(max_concurrent: int = 5):
    """
    Run image enhancement workflows for multiple images.
    """
    # Get Temporal server configuration
    temporal_address = os.getenv('TEMPORAL_ADDRESS', 'localhost:7233')
    temporal_namespace = os.getenv('TEMPORAL_NAMESPACE', 'default')
    task_queue = os.getenv('TEMPORAL_TASK_QUEUE', 'image-enhancement-queue')
    
    # Get batch processing configuration
    images_config = os.getenv('IMAGES_TO_PROCESS', '')
    enhancement_prompt = os.getenv('ENHANCEMENT_PROMPT', 
                                 'Make this image more vibrant, increase clarity and sharpness, improve lighting')
    
    # Parse image list
    images = parse_image_list(images_config)
    
    if not images:
        # Fall back to single image configuration
        source_bucket = os.getenv('SOURCE_BUCKET', 'ricardotemporal')
        source_key = os.getenv('SOURCE_KEY', 'funny.png')
        dest_bucket = os.getenv('DEST_BUCKET', 'ricardotemporalprocessed')
        dest_key = os.getenv('DEST_KEY', 'enhanced_funny.png')
        
        images = [{
            "source_bucket": source_bucket,
            "source_key": source_key,
            "dest_bucket": dest_bucket,
            "dest_key": dest_key
        }]
    
    # AWS and OpenAI configuration
    config = ImageProcessingConfig(
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
        aws_region=os.getenv('AWS_REGION', 'us-east-1'),
        openai_api_key=os.getenv('OPENAI_API_KEY')
    )
    
    # Debug: Check if credentials are loaded
    logger.info(f"AWS Region: {config.aws_region}")
    logger.info(f"AWS Access Key ID loaded: {'Yes' if config.aws_access_key_id else 'No'}")
    logger.info(f"AWS Secret Key loaded: {'Yes' if config.aws_secret_access_key else 'No'}")
    logger.info(f"OpenAI API Key loaded: {'Yes' if config.openai_api_key else 'No'}")
    logger.info(f"Processing {len(images)} images with max concurrency: {max_concurrent}")
    
    try:
        # Connect to Temporal server
        client = await Client.connect(temporal_address, namespace=temporal_namespace)
        
        # Process images in batches
        results = []
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_with_semaphore(image_config):
            async with semaphore:
                return await run_single_image_workflow(
                    client, config, image_config, task_queue, enhancement_prompt
                )
        
        # Start all workflows
        logger.info("Starting all workflows...")
        start_time = time.time()
        
        tasks = [process_with_semaphore(image_config) for image_config in images]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_duration = round(end_time - start_time, 2)
        
        # Process results
        successful = 0
        failed = 0
        
        for i, result in enumerate(results):
            if isinstance(result, BaseException):
                logger.error(f"Workflow {i+1} failed with exception: {result}")
                failed += 1
            elif result.get('status') == 'success':
                logger.info(f"‚úÖ {result['source']} -> {result['destination']} ({result['duration_seconds']}s)")
                successful += 1
            else:
                logger.error(f"‚ùå {result['source']}: {result.get('error', 'Unknown error')}")
                failed += 1
        
        # Summary
        logger.info(f"\n" + "="*80)
        logger.info(f"BATCH PROCESSING SUMMARY")
        logger.info(f"="*80)
        logger.info(f"Total images: {len(images)}")
        logger.info(f"Successful: {successful}")
        logger.info(f"Failed: {failed}")
        logger.info(f"Total duration: {total_duration}s")
        logger.info(f"Average per image: {round(total_duration/len(images), 2)}s")
        logger.info(f"="*80)
        
        return results
        
    except Exception as e:
        logger.error(f"Error running batch workflows: {e}")
        raise

async def main():
    """
    Main function to run the workflows.
    """
    try:
        # Get max concurrent workflows from environment
        max_concurrent = int(os.getenv('MAX_CONCURRENT_WORKFLOWS', '5'))
        
        results = await run_batch_image_workflows(max_concurrent)
        
        # Count successes and failures
        successful = sum(1 for r in results if isinstance(r, dict) and r.get('status') == 'success')
        total = len(results)
        
        if successful == total:
            print(f"\nüéâ All {total} images processed successfully!")
        else:
            failed = total - successful
            print(f"\n‚ö†Ô∏è  Processed {successful}/{total} images successfully ({failed} failed)")
            exit(1)
            
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())

